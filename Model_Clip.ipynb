{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Imagnamename
    "import cnamename
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and processor\n",
    "model = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate=2):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(video_capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    interval = int(fps / frame_rate)\n",
    "    frame_number = 0\n",
    "    frames = []\n",
    "    while True:\n",
    "        success, frame = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        if frame_number % interval == 0:\n",
    "            frames.append(frame)\n",
    "\n",
    "        frame_number += 1\n",
    "    video_capture.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offsetImagePath = \"./offset1.jpg\"\n",
    "offsetFeatures = []\n",
    "inputs = processor(images=Image.open(offsetImagePath), return_tensors=\"pt\", padding=True)\n",
    "with torch.no_grad():\n",
    "    offsetFeatures = model(**inputs)\n",
    "# print(offsetFeatures)\n",
    "offsetFeatures = offsetFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "def load_embeddings():\n",
    "    path_to_videos = \"./sourceDataVideos/\"\n",
    "    videos = os.listdir(path_to_videos)\n",
    "    # videos = videos[:2]\n",
    "    print(videos)\n",
    "    count = 0\n",
    "    for video in videos:\n",
    "        print(\"--------------------- Processing video: \", count, video)\n",
    "        count += 1\n",
    "        video_path = os.path.join(path_to_videos, video)\n",
    "        frames = extract_frames(video_path, 20)\n",
    "        id = video.split(\"_\")[0]\n",
    "        frame_number = 0\n",
    "        for frame in frames:\n",
    "            image = Image.fromarray(frame)\n",
    "            inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "            with torch.no_grad():\n",
    "                features = model(**inputs)\n",
    "                features.image_embeds = features.image_embeds - offsetFeatures.image_embeds\n",
    "            embeddings[id + \"_\"+str(frame_number)] = features\n",
    "            frame_number += 1\n",
    "load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = {}\n",
    "def load_products():\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv('output.csv')\n",
    "\n",
    "    # Create the dictionary\n",
    "    for _, row in df.iterrows():\n",
    "        product_id = str(row['id'])\n",
    "        products[product_id] = {\n",
    "            'id': product_id,\n",
    "            'nickname': row['nickname'],\n",
    "            'price': row['price']\n",
    "        }\n",
    "load_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {}\n",
    "def load_images():\n",
    "    path_to_images = \"./sourceDataImages/\"\n",
    "    imageDirs = os.listdir(path_to_images)\n",
    "    print(imageDirs)\n",
    "    for imageDir in imageDirs:\n",
    "        try:\n",
    "            imagePaths = os.listdir(os.path.join(path_to_images, imageDir))\n",
    "            count = 0\n",
    "            print(\"Processing imageDir: \", imageDir)\n",
    "            for imagePath in imagePaths:\n",
    "                image = Image.open(os.path.join(path_to_images, imageDir, imagePath))\n",
    "                inputs = processor(images=image, return_tensors=\"pt\", padding=True)\n",
    "                with torch.no_grad():\n",
    "                    features = model(**inputs)\n",
    "                    features.image_embeds = features.image_embeds - offsetFeatures.image_embeds\n",
    "                images[imageDir + \"_\" + str(count) ] = features\n",
    "                count += 1\n",
    "        except:\n",
    "            print(\"Error processing imageDir: \", imageDir)\n",
    "load_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchProduct(imageFeatures):\n",
    "    # Process the input image\n",
    "    query_embedding = imageFeatures.image_embeds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = []\n",
    "    for id, embedding in embeddings.items():\n",
    "        embedding = embedding.image_embeds.detach().cpu().numpy()\n",
    "        similarity = cosine_similarity(query_embedding, embedding)\n",
    "        similarities.append((id, similarity))\n",
    "\n",
    "    top_five = sorted(similarities, key=lambda item: item[1], reverse=True)[:5]\n",
    "    # filtered_results = [(id, sim) for id, sim in top_five if sim > 0.8]\n",
    "    # Get the product details for the top five items\n",
    "    filtered_products = []\n",
    "    for closest_id, maxSimilarity in top_five:\n",
    "        closest_id = closest_id.split(\"_\")[0]\n",
    "        product_details = products[closest_id]\n",
    "        filtered_products.append((product_details, maxSimilarity))\n",
    "\n",
    "    return filtered_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = 0\n",
    "for id, feature in images.items():\n",
    "    product_id = id.split(\"_\")[0]\n",
    "    query_product = products[product_id]\n",
    "    results = searchProduct(feature)\n",
    "    print(\"Query: \", query_product, results)\n",
    "    \n",
    "    if query_product['id'] == results[0][0]['id'] or query_product['id'] == results[1][0]['id'] or query_product['id'] == results[2][0]['id'] or query_product['id'] == results[3][0]['id'] or query_product['id'] == results[4][0]['id']:\n",
    "        match += 1\n",
    "    print(query_product['nickname'], \"-------\", results[0][0]['nickname'])\n",
    "print(\"Match: \", match, \"Total: \", len(images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
