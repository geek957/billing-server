{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from lang_sam import LangSAM\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configs\n",
    "# path to directory to dump embeddings\n",
    "run_id = 'run_1'\n",
    "# number of frames to extract in a minute\n",
    "frame_rate = 2\n",
    "#maxID : number of products to process starting: 1001\n",
    "max_id = 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_type() -> str:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return \"cpu\"\n",
    "    elif torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    else:\n",
    "        # logging.warning(\"No GPU found, using CPU instead\")\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and processor\n",
    "embeddingModel = CLIPVisionModelWithProjection.from_pretrained(\"openai/clip-vit-base-patch16\").half().to(get_device_type())\n",
    "embeddingProcessor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetector:\n",
    "    def __init__(self):\n",
    "        self.model = LangSAM(sam_type=\"sam2.1_hiera_tiny\", gdino_type=\"tiny\")\n",
    "    \n",
    "    def predict(self, image_pil, text_prompt):\n",
    "        results = self.model.predict([image_pil], [text_prompt])\n",
    "        return results\n",
    "    \n",
    "    def plot_results(self, image_pil, results, text_prompt):\n",
    "        \n",
    "        # Convert the mask to a numpy array\n",
    "        mask = results[0]['masks'][0]\n",
    "\n",
    "        # Plot the image\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image_pil)\n",
    "\n",
    "        # Plot the bounding box\n",
    "        box = results[0]['boxes'][0]\n",
    "        plt.gca().add_patch(plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], edgecolor='red', facecolor='none', linewidth=2))\n",
    "\n",
    "        # Plot the mask\n",
    "        plt.imshow(np.ma.masked_where(mask == 0, mask), alpha=0.5, cmap='jet')\n",
    "\n",
    "        plt.title(f\"Prediction for: {text_prompt}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # given an image and results, hide the mask on the image by making the values zero within the mask\n",
    "    def hide_mask(self, image_pil, results):\n",
    "        mask = results[0]['masks'][0]\n",
    "        image_np = np.array(image_pil)\n",
    "        image_np[mask == 1] = 0\n",
    "        return Image.fromarray(image_np)\n",
    "\n",
    "    def crop_image(self, image_pil, results):\n",
    "        box = results[0]['boxes'][0]\n",
    "        cropped_image = image_pil.crop((box[0], box[1], box[2], box[3]))\n",
    "        return cropped_image\n",
    "    \n",
    "    def predict_and_crop_image(self, image_pil, text_prompt):\n",
    "        results = self.predict(image_pil, text_prompt)\n",
    "        cropped_image = self.crop_image(image_pil, results)\n",
    "        return cropped_image\n",
    "\n",
    "    def show_image(self, image_pil):\n",
    "        plt.imshow(image_pil)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    def resize(self, image_pil, long_side):\n",
    "        width, height = image_pil.size\n",
    "        if width > height:\n",
    "            new_width = long_side\n",
    "            new_height = int(long_side * height / width)\n",
    "        else:\n",
    "            new_height = long_side\n",
    "            new_width = int(long_side * width / height)\n",
    "        return image_pil.resize((new_width, new_height))\n",
    "object_detector = ObjectDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "    interval = int(fps / frame_rate)\n",
    "    frame_number = 0\n",
    "    frames = []\n",
    "    while True:\n",
    "        success, frame = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "        if frame_number % interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "\n",
    "        frame_number += 1\n",
    "    video_capture.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toFilter(path, maxNumber= 1999):\n",
    "    try:\n",
    "        number = path.split('_')[0]\n",
    "        return int(number) <= maxNumber\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "video_frame_map = {}\n",
    "video_frame_map_cropped = {}\n",
    "video_frame_map_masked = {}\n",
    "def load_embeddings():\n",
    "    path_to_videos = \"./sourceDataVideos/\"\n",
    "    videos = os.listdir(path_to_videos)\n",
    "    # videos = videos[:2]\n",
    "    print(videos)\n",
    "    count = 0\n",
    "    for video in videos:\n",
    "        try:\n",
    "            if(not toFilter(video, max_id)):\n",
    "                continue\n",
    "            print(\"--------------------- Processing video: \", count, video)\n",
    "            count += 1\n",
    "            video_path = os.path.join(path_to_videos, video)\n",
    "            frames = extract_frames(video_path, frame_rate)\n",
    "            print(\"Number of frames: \", len(frames))\n",
    "            id = video.split(\"_\")[0]\n",
    "            frame_number = 0\n",
    "            for frame in frames:\n",
    "                print(\"Processing frame: \", frame_number)\n",
    "                image = Image.fromarray(frame)\n",
    "                resultsMasked = object_detector.predict(image, \"hand, no background, no object\")\n",
    "                masked_image = object_detector.hide_mask(image, resultsMasked)\n",
    "                cropped_image = object_detector.predict_and_crop_image(masked_image, \"object, no hand, no background\")\n",
    "                inputs = embeddingProcessor(images=cropped_image, return_tensors=\"pt\", padding=True).to(get_device_type())\n",
    "                with torch.no_grad():\n",
    "                    features = embeddingModel(**inputs)\n",
    "                    features.image_embeds = features.image_embeds\n",
    "                embeddings[id + \"_\" + str(frame_number)] = features\n",
    "                video_frame_map[id + \"_\" + str(frame_number)] = image\n",
    "                # video_frame_map_cropped[id + \"_\" + str(frame_number)] = cropped_image\n",
    "                # video_frame_map_masked[id + \"_\" + str(frame_number)] = masked_image\n",
    "                frame_number += 1\n",
    "        except Exception as e:\n",
    "            print(\"failed processing video\",video, e)\n",
    "load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = {}\n",
    "def load_products():\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv('output.csv')\n",
    "\n",
    "    # Create the dictionary\n",
    "    for _, row in df.iterrows():\n",
    "        product_id = str(row['id'])\n",
    "        products[product_id] = {\n",
    "            'id': product_id,\n",
    "            'nickname': row['nickname'],\n",
    "            'price': row['price']\n",
    "        }\n",
    "load_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = {}\n",
    "image_map = {}\n",
    "image_cropped_map = {}\n",
    "image_masked_map = {}\n",
    "def load_images():\n",
    "    path_to_images = \"./sourceDataImages/\"\n",
    "    imageDirs = os.listdir(path_to_images)\n",
    "    print(imageDirs)\n",
    "    imageDirCount = 0\n",
    "    for imageDir in imageDirs:\n",
    "        try:\n",
    "            if(not toFilter(imageDir, max_id)):\n",
    "                continue\n",
    "            print(\"Processing imageDir: \", imageDirCount)\n",
    "            imagePaths = os.listdir(os.path.join(path_to_images, imageDir))\n",
    "            count = 0\n",
    "            print(\"Processing imageDir: \", imageDir)\n",
    "            for imagePath in imagePaths:\n",
    "                image = Image.open(os.path.join(path_to_images, imageDir, imagePath))\n",
    "                resultsMasked = object_detector.predict(image, \"hand, no background, no object\")\n",
    "                masked_image = object_detector.hide_mask(image, resultsMasked)\n",
    "                cropped_image = object_detector.predict_and_crop_image(masked_image, \"object, no hand, no background\")\n",
    "                inputs = embeddingProcessor(images=cropped_image, return_tensors=\"pt\", padding=True).to(get_device_type())\n",
    "                with torch.no_grad():\n",
    "                    features = embeddingModel(**inputs)\n",
    "                    features.image_embeds = features.image_embeds\n",
    "                images[imageDir + \"_\" + str(count) ] = features\n",
    "                image_map[imageDir + \"_\" + str(count) ] = image\n",
    "                # image_cropped_map[imageDir + \"_\" + str(count) ] = cropped_image\n",
    "                # image_masked_map[imageDir + \"_\" + str(count)] = masked_image\n",
    "                count += 1\n",
    "        except:\n",
    "            print(\"Error processing imageDir: \", imageDir)\n",
    "        imageDirCount += 1\n",
    "        \n",
    "load_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchProduct(imageFeatures):\n",
    "    # Process the input image\n",
    "    if(get_device_type() == 'cuda'):\n",
    "        imageFeatures.image_embeds = imageFeatures.image_embeds.to(torch.float32)\n",
    "    query_embedding = imageFeatures.image_embeds.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = []\n",
    "    for id, embedding in embeddings.items():\n",
    "        if(get_device_type() == 'cuda'):\n",
    "            embedding.image_embeds = embedding.image_embeds.to(torch.float32)\n",
    "        embedding = embedding.image_embeds.to(torch.float32).detach().cpu().numpy()\n",
    "        similarity = cosine_similarity(query_embedding, embedding)\n",
    "        similarities.append((id, similarity))\n",
    "\n",
    "    top_five = sorted(similarities, key=lambda item: item[1], reverse=True)[:5]\n",
    "    # filtered_results = [(id, sim) for id, sim in top_five if sim > 0.8]\n",
    "    # Get the product details for the top five items\n",
    "    filtered_products = []\n",
    "    for closest_id, maxSimilarity in top_five:\n",
    "        # closest_id = closest_id.split(\"_\")[0]\n",
    "        product_details = products[closest_id.split(\"_\")[0]]\n",
    "        filtered_products.append((product_details, closest_id, maxSimilarity))\n",
    "\n",
    "    return filtered_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = 0\n",
    "for id, feature in images.items():\n",
    "    product_id = id.split(\"_\")[0]\n",
    "    query_product = products[product_id]\n",
    "    results = searchProduct(feature)\n",
    "    print(\"Query: \", query_product, results)\n",
    "    \n",
    "    if query_product['id'] == results[0][0]['id']:\n",
    "    # or query_product['id'] == results[1][0]['id'] or query_product['id'] == results[2][0]['id'] or query_product['id'] == results[3][0]['id'] or query_product['id'] == results[4][0]['id']:\n",
    "        match += 1\n",
    "    \n",
    "    print(query_product['nickname'], \"-------\", results[0][0]['nickname'])\n",
    "print(\"Match: \", match, \"Total: \", len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match = 0\n",
    "for id, feature in images.items():\n",
    "    product_id = id.split(\"_\")[0]\n",
    "    query_product = products[product_id]\n",
    "    results = searchProduct(feature)\n",
    "    print(\"Query: \", query_product, results)\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Plot images in a 2x1 matrix\n",
    "    if product_id != results[0][0]['id']:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "        # Plot the query image\n",
    "        query_image = image_map[id]\n",
    "        axes[0].imshow(query_image)\n",
    "        axes[0].set_title(f\"Query: {query_product['nickname']}\", fontsize=6)\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        # query_crop_image = image_cropped_map[id]\n",
    "        # axes[1].imshow(query_crop_image)\n",
    "        # axes[1].set_title(f\"QueryCropped: {query_product['nickname']}\", fontsize=6)\n",
    "        # axes[1].axis('off')\n",
    "\n",
    "\n",
    "        # query_masked_image = image_masked_map[id]\n",
    "        # axes[2].imshow(query_masked_image)\n",
    "        # axes[2].set_title(f\"QueryMasked: {query_product['nickname']}\", fontsize=6)\n",
    "        # axes[2].axis('off')\n",
    "        # Plot the matched video frame\n",
    "        if product_id == results[0][0]['id']:\n",
    "            matchString = \"Matched\"\n",
    "        else:\n",
    "            matchString = \"Not Matched\"\n",
    "        matched_id = results[0][1]\n",
    "        matched_frame = video_frame_map[matched_id]\n",
    "        axes[1].imshow(matched_frame)\n",
    "        axes[1].set_title(f\"{matchString}: {results[0][0]['nickname']}\", fontsize=6)\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        # axes[4].imshow(video_frame_map_masked[matched_id])\n",
    "        # axes[4].set_title(f\"{matchString}: {results[0][0]['nickname']}\", fontsize=6)\n",
    "        # axes[4].axis('off')\n",
    "\n",
    "        # axes[5].imshow(video_frame_map_masked[matched_id])\n",
    "        # axes[5].set_title(f\"{matchString}: {results[0][0]['nickname']}\", fontsize=6)\n",
    "        # axes[5].axis('off')\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "    if product_id == results[0][0]['id']:\n",
    "        match += 1\n",
    "    \n",
    "    print(query_product['nickname'], \"-------\", results[0][0]['nickname'])\n",
    "print(\"Match: \", match, \"Total: \", len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_to_dump = {\n",
    "    'embeddings': embeddings,\n",
    "    'video_frame_map': video_frame_map,\n",
    "    'video_frame_map_cropped': video_frame_map_cropped,\n",
    "    'video_frame_map_masked': video_frame_map_masked,\n",
    "    'products': products,\n",
    "    'images': images,\n",
    "    'image_map': image_map,\n",
    "    'image_cropped_map': image_cropped_map\n",
    "}\n",
    "\n",
    "if os.path.exists(run_id):\n",
    "    print(\"Directory already exists, skipping saving variables\")\n",
    "    exit()\n",
    "\n",
    "if not os.path.exists(run_id):\n",
    "    os.makedirs(run_id)\n",
    "\n",
    "for name, variable in variables_to_dump.items():\n",
    "    torch.save(variable, os.path.join(run_id, name + \".pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "billing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
